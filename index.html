<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 6.2.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="/categories">Categories</a>
        
          <a class="main-nav-link" href="/tags">Tags</a>
        
          <a class="main-nav-link" href="/about">About</a>
        
      </nav>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-eagle" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/20/eagle/" class="article-date">
  <time class="dt-published" datetime="2022-06-20T06:45:03.000Z" itemprop="datePublished">2022-06-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/20/eagle/">eagle使用功能</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Kafka Eagle监控管理系统，提供了一个可视化页面，使用者可以拥有不同的角色，例如管理员、开发者、游客等。不同的角色对应不同的使用权限。在知道了Kafka Eagle的作用之后，那么它包含哪些功能呢？核心功能如下所示:<br><img src="/../images/%E5%9B%BE%E7%89%8749.png"></p>
<p>数据面板<br> 负责展示Kafka集群的Broker数、Topic数、Consumer数、以及Topic LogSize Top10和Topic Capacity Top10数据。<br><img src="/../images/%E5%9B%BE%E7%89%8750.png"></p>
<p>主题<br> 该模块包含主题创建、主题管理、主题预览、KSQL查询主题、主题数据写入、主题属性配置等。<br> <img src="/../images/%E5%9B%BE%E7%89%8751.png"></p>
<p>消费者组<br> 该模块包含监控不同消费者组中的Topic被消费的详情，例如LogSize、Offsets、以及Lag等。同时，支持查看Lag的历史趋势图。<br><img src="/../images/%E5%9B%BE%E7%89%8752.png"></p>
<p>集群管理<br> 该模块包含Kafka集群和Zookeeper集群的详情展示，例如Kafka的IP和端口、版本号、启动时间、Zookeeper的Leader和Follower。同时，还支持多Kafka集群切换，以及Zookeeper Client数据查看等功能。<br><img src="/../images/%E5%9B%BE%E7%89%8753.png"></p>
<p>指标监控<br> 该模块包含监控Kafka集群和Zookeeper集群的核心指标，包含Kafka的消息发送趋势、消息大小接收与发送趋势、Zookeeper的连接数趋势等。同时，还支持查看Broker的瞬时指标数据。<br> <img src="/../images/%E5%9B%BE%E7%89%8754.png"></p>
<p>告警<br> 该模块包含告警集群异常和消费者应用Lag异常。同时，支持多种IM告警方式，例如邮件、钉钉、微信、Webhook等。<br> <img src="/../images/%E5%9B%BE%E7%89%8755.png"></p>
<p>系统管理<br> 该模块包含用户管理，例如创建用户、用户授权、资源管理等<br> <img src="/../images/%E5%9B%BE%E7%89%8756.png"></p>
<p>数据大屏<br> 该模块包含展示消费者和生产者当日及最近7天趋势、Kafka集群读写速度、Kafka集群历史总记录等。<br>  <img src="/../images/%E5%9B%BE%E7%89%8757.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/20/eagle/" data-id="cl4me0ao10000gwv987n5gkou" data-title="eagle使用功能" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kafka-eagle" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/19/kafka-eagle/" class="article-date">
  <time class="dt-published" datetime="2022-06-19T08:20:48.000Z" itemprop="datePublished">2022-06-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/19/kafka-eagle/">kafka-eagle配置文件</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p> 一、在官网下载安装包：<a target="_blank" rel="noopener" href="https://www.kafka-eagle.org/">https://www.kafka-eagle.org/</a></p>
<p>二、上传压缩包</p>
<p>三、解压压缩包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line">tar -zxvf kafka-eagle-bin-2.1.0.tar.gz -C </span><br></pre></td></tr></table></figure>

<p>四、建立软链接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/ kafka-eagle-bin-2.1.0 /export/server/kafka-eagle</span><br></pre></td></tr></table></figure>

<p>五、在kafka-eagle目录下使用命令tar -zxvf efak-web-2.1.0-bin.tar.gz解压该文件包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/kafka-eagle/</span><br><span class="line">tar -zxvf efak-web-2.1.0-bin.tar.gz</span><br></pre></td></tr></table></figure>

<p>六、配置环境变量:JAVA_HOME 和 KE_HOME</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> KE_HOME=/export/server/kafka-eagle</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$KE_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p>七、配置kafka-eagle</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/kafka-eagle/conf </span><br><span class="line">vi system-config.properties</span><br></pre></td></tr></table></figure>

<p>修改内容为</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka.eagle.zk.cluster.alias=cluster1</span><br><span class="line">cluster1.zk.list=node1:2181,node2:2181,node3:2181</span><br><span class="line">cluster1.kafka.eagle.broker.size=3</span><br><span class="line">kafka.eagle.url=jdbc:sqlite:/export/data/db/ke.db</span><br></pre></td></tr></table></figure>

<p>八、启动前需要手动创建&#x2F;export&#x2F;data&#x2F;db目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /export/data/db</span><br></pre></td></tr></table></figure>

<p>九、启动eagle<br>（启动之前应先启动zookeeper和kafka）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/kafka-eagle/efak-web-2.1.0/bin/ke.sh start</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%8748.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/19/kafka-eagle/" data-id="cl4me0aoi0007gwv9eyvt1fws" data-title="kafka-eagle配置文件" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kafka-3" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/05/kafka-3/" class="article-date">
  <time class="dt-published" datetime="2022-06-05T07:19:29.000Z" itemprop="datePublished">2022-06-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/05/kafka-3/">kafka API使用方法</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一.生产者API</p>
<p>1.新建Maven项目，配置pom.xml<br><img src="/../images/%E5%9B%BE%E7%89%8732.png"><br><img src="/../images/%E5%9B%BE%E7%89%8733.png"></p>
<p>2.新建ProducerDemo类，ProducerCallbackDemo类<br><img src="/../images/%E5%9B%BE%E7%89%8734.png"><br><img src="/../images/%E5%9B%BE%E7%89%8735.png"><br><img src="/../images/%E5%9B%BE%E7%89%8736.png"></p>
<p>3.生产者原理<br><img src="/../images/%E5%9B%BE%E7%89%8737.png"></p>
<p>（1）Kafka生产者有两个线程，一个为主线程，一个为Sender线程（它是一个守护线程）<br>（2）消息发送后首先经历拦截器，在拦截器中可以对消息做一些统一的操作，比如：加上统一的标识，然后会被序列化为字节流<br>（3）到达分区器，分区器主要是对消息去哪个分区做规划<br>（4）做好分区规划后，消息到达了消息累加器，累加器是一个双端队列（每一个队列对应一个分区），然后将消息封装到ProducerBatch,ProducerBatch也是一个多消息的容器，它是可以不断想里面加入消息的。让消息变成一批次，一批次的。<br>（5）Sender线程主动从消息累加器中取消息，然后将上一步Deque&lt;partitionId,ProducerBatch&gt;转换为&lt;Node,List&gt;的形式，然后再将其转换为&lt;Node,Request&gt;,这一步是将Kafka消息从逻辑上转换为物理上的主要转折点。从这步开始，就不会关心分区这个逻辑概念，只会注意要发向哪一台机器了<br>（6）在发送之前，还会将消息转换为Map&lt;Node,Dequen【Requst】&gt;<br>（7）保存在InFlightRequst中，表示消息发送等待回应<br>（8）消息发最终由Selector发送</p>
<p>4.重要的生产者参数<br>（1）acks<br>这个参数主要用来指定分区中必须要有多少个副本都收到这个消息，生产者客户端才认为这条消息成功写入。它涉及消息的可靠性和吞吐量之间的权衡。acks参数有3种类型的值（都是字符串类型）<br>（2）max.request.size<br>这个参数用来限制生产者客户端所能发送消息的最大值<br>（3）retry.backoff.ms和<br>retries 参数用来配置生产者重试的次数，默认值为0，也就是说在发生异常的时候不进行重试。<br>（4）compression.type<br>这个参数用来指定消息的压缩放方式，默认值是“none”,即默认情况下，消息不会被压缩。<br>（5）connections. max.ides.ms<br>这个参数用来指定在多久之后关闭闲置连接<br>（6）linger.ms<br>这个参数用来指定生产者发送 ProducerBatch 之前等待更多的消息（ProducerRecord）加入 ProducerBatch 的时间<br>（7）receive.buffer.bytes<br>这个参数用来设置 Socket 接收消息缓冲区（SO_RECBUF）的大小<br>（8）send.buffer.bytes<br>这个参数用来设置 Socket 发送消息缓冲区（SO_SNDBUF）的大小<br>（9）request.timeout.ms<br>这个参数用来配置 Producer 等待请求响应的最长时</p>
<p>二.消费者API</p>
<p>1.一个正常的消费逻辑需要具备以下几个步骤:<br>(1)  配置消费者客户端参数；<br>(2)  创建相应的消费者实例;<br>(3)  订阅主题;<br>(4)  拉取消息并消费;<br>(5)  提交消费位移 offset;<br>(6)  关闭消费者实例。</p>
<p>2.subscribe 有如下重载方法:<br><img src="/../images/%E5%9B%BE%E7%89%8738.png"></p>
<p>3.正则方式订阅主题<br>如果消费者采用的是正则表达式的方式(subscribe(Pattern))订阅, 在之后的过程中,如果有人又创建了新的主题,并且主题名字与正表达式相匹配,那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题,并且可以处理不同的类型,那么这种订阅方式就很有效。</p>
<p>4.assign 订阅主题<br>这个方法只接受参数 partitions,用来指定需要订阅的分区集合</p>
<p>5.subscribe 与 assign 的区别<br>(1)  通过 subscribe()方法订阅主题具有消费者自动再均衡功能 ;<br> 在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。<br> 当消费组的消费者增加或减少时,分区分配关系会自动调整,以实现消费负载均衡及故障自动转移。<br>(2)  assign() 方法订阅分区时,是不具备消费者自动均衡的功能的;<br>其实这一点从 assign()方法参数可以看出端倪,两种类型 subscribe()都有<br>ConsumerRebalanceListener 类型参数的方法,而 assign()方法却没有。</p>
<p>6.消息的消费模式<br>Kafka 中的消费是基于拉取模式的。消息的消费一般有两种模式:推送模式和拉取模式。<br>推模式是服务端主动将消息推送给消费者,而拉模式是消费者主动向服务端发起请求来拉取消息<br>Kafka 中的消息消费是一个不断轮询的过程,消费者所要做的就是重复地调用 poll() 方法, poll() 方法返回的是所订阅的主题(分区)上的一组消息。</p>
<p>7.指定位移消费<br>seek() 方法:从特定的位移处开始拉取消息</p>
<p>8.再均衡监听器<br>一个消费组中,一旦有消费者的增减发生,会触发消费者组的 rebalance 再均衡;<br>如果 A 消费者消费掉的一批消息还没来得及提交 offset, 而它所负责的分区在 rebalance 中转移给了 B 消费者,则有可能发生数据的重复消费处理。此情形下,可以通过再均衡监听器做一定程度的补救;</p>
<p>9.自动位移提交<br>Kafka 消费的编程逻辑中位移提交是一大难点,自动提交消费位移的方式非常简便,它免去了复杂的位移提交逻辑,让编码更简洁。但随之而来的是重复消费和消息丢失的问题。<br>(1)  重复消费<br>假设刚刚提交完一次消费位移,然后拉取一批消息进行消费,在下一次自动提交消费位移之前,消费者崩溃了,那么又得从上一次位移提交的地方重新开始消费,这样便发生了重复消费的现象(对于再均衡的情况同样适用)。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小,但这样并不能避免重复消费的发送,而且也会使位移提交更加频繁。<br>(2)  丢失消息<br>拉取线程不断地拉取消息并存入本地缓存, 比如在 BlockingQueue 中, 另一个处理线程从缓存中读取消息并进行相应的逻辑处理</p>
<p>10.新建ConsumerDemo，ConsumerDemo1，ConsumerTask，ConsumerDemo2，ConsumerSeekOffset类<br><img src="/../images/%E5%9B%BE%E7%89%8739.png"><br><img src="/../images/%E5%9B%BE%E7%89%8740.png"></p>
<p>三.Topic管理API</p>
<p>1.KafkaAdminClient 不仅可以用来管理 broker、配置和 ACL (Access Control List),还可用来管理主题)它提供了以下方法:<br><img src="/../images/%E5%9B%BE%E7%89%8741.png"></p>
<p>2.连接到kafka服务器<br><img src="/../images/%E5%9B%BE%E7%89%8742.png"></p>
<p>3.创建管理对象，使用get方法获取主题名称<br><img src="/../images/%E5%9B%BE%E7%89%8743.png"></p>
<p>4.查看topic具体信息<br><img src="/../images/%E5%9B%BE%E7%89%8744.png"></p>
<p>5.创建topic<br><img src="/../images/%E5%9B%BE%E7%89%8745.png"></p>
<p>6.运行代码<br>（1）topic列表获取成功<br>tpc_1，tpc_2信息获取成功<br><img src="/../images/%E5%9B%BE%E7%89%8746.png"></p>
<p>（2）topic创建成功<br><img src="/../images/%E5%9B%BE%E7%89%8747.png"></p>
<p>7.producer 发布消息<br>（1）写入方式<br>producer 采用 push 模式将消息发布到 broker，每条消息都被 append 到 patition 中，属于顺序写磁盘（顺序写磁盘效率比随机写内存要高，保障 kafka 吞吐率）。<br>（2）消息路由<br>producer 发送消息到 broker 时，会根据分区算法选择将其存储到哪一个 partition。其路由机制为：<br>指定了 patition，则直接使用；<br>未指定 patition 但指定 key，通过对 key 的 value 进行hash 选出一个 patition<br>patition 和 key 都未指定，使用轮询选出一个 patition。<br>（3）写入流程<br>流程说明：<br>producer 先从 zookeeper 的 “&#x2F;brokers&#x2F;…&#x2F;state” 节点找到该 partition 的 leader<br>producer 将消息发送给该 leader<br>leader 将消息写入本地 log<br>followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK<br>leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</p>
<p>8.创建topic<br>流程说明：<br>（1） controller 在 ZooKeeper 的 &#x2F;brokers&#x2F;topics 节点上注册 watcher，当 topic 被创建，则 controller 会通过 watch 得到该 topic 的 partition&#x2F;replica 分配。<br>（2） controller从 &#x2F;brokers&#x2F;ids 读取当前所有可用的 broker 列表，对于 set_p 中的每一个 partition：<br>从分配给该 partition 的所有 replica（称为AR）中任选一个可用的 broker 作为新的 leader，并将AR设置为新的ISR将新的 leader 和 ISR 写入 &#x2F;brokers&#x2F;topics&#x2F;[topic]&#x2F;partitions&#x2F;[partition]&#x2F;state<br>（3） controller 通过 RPC 向相关的 broker 发送 LeaderAndISRRequest。</p>
<p>9.删除topic<br>（1） controller 在 zooKeeper 的 &#x2F;brokers&#x2F;topics 节点上注册 watcher，当 topic 被删除，则 controller 会通过 watch 得到该 topic 的 partition&#x2F;replica 分配。<br>（2）若 delete.topic.enable&#x3D;false，结束；否则 controller 注册在 &#x2F;admin&#x2F;delete_topics 上的 watch 被 fire，controller 通过回调向对应的 broker 发送 StopReplicaRequest。</p>
<p>10.consumer API<br>kafka 提供了两套 consumer API：<br>（1）The high-level Consumer API<br>（2）The SimpleConsumer API<br>其中 high-level consumer API 提供了一个从 kafka 消费数据的高层抽象，而 SimpleConsumer API 则需要开发人员更多地关注细节。<br>The high-level consumer API提供了 consumer group 的语义，一个消息只能被 group 内的一个 consumer 所消费，且 consumer 消费消息时不关注 offset，最后一个 offset 由 zookeeper 保存。<br>使用 high-level consumer API 可以是多线程的应用，应当注意：<br>（1）如果消费线程大于 patition 数量，则有些线程将收不到消息<br>（2）如果 patition 数量大于线程数，则有些线程多收到多个 patition 的消息<br>（3）如果一个线程消费多个 patition，则无法保证你收到的消息的顺序，而一个 patition 内的消息是有序的。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/05/kafka-3/" data-id="cl4me0aoj0008gwv90ioca68w" data-title="kafka API使用方法" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kafka-2" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/05/kafka-2/" class="article-date">
  <time class="dt-published" datetime="2022-06-05T07:00:34.000Z" itemprop="datePublished">2022-06-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/05/kafka-2/">Kafka命令行操作</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>1.查看当前可用topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/kafka/bin/</span><br><span class="line">kafka-topics.sh --list --zookeeper node1:2181 _consumer_offsets</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%8726.png"></p>
<p>2.创建topic，检查是否创建成功</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --create --topic tpc_1 --partitions 2 --replication-factor 2 --zookeeper node1:2181</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%8727.png"></p>
<p>3.进入cd &#x2F;export&#x2F;data&#x2F;kafka-logs路径下查看分区的分布<br><img src="/../images/%E5%9B%BE%E7%89%8728.png"></p>
<p>4.删除topic</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh  --delete --topic tpc_1 --zookeeper node1:2181</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%8729.png"></p>
<p>5.查看topic、修改参数<br><img src="/../images/%E5%9B%BE%E7%89%8730.png"></p>
<p>6.生产者写入数据、消费者拉取数据<br><img src="/../images/%E5%9B%BE%E7%89%8731.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/05/kafka-2/" data-id="cl4me0aog0005gwv933rjfahj" data-title="Kafka命令行操作" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-kafka-1" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/06/05/kafka-1/" class="article-date">
  <time class="dt-published" datetime="2022-06-05T05:47:24.000Z" itemprop="datePublished">2022-06-05</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/06/05/kafka-1/">kafka 环境配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>·上传文件包到  &#x2F;export&#x2F;server&#x2F;</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br></pre></td></tr></table></figure>

<p>·解压文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf kafka_2.11-2.0.0.tgz</span><br></pre></td></tr></table></figure>

<p>·创建软连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s kafak_2.11-2.0.0/ kafka</span><br></pre></td></tr></table></figure>

<p>·进入 &#x2F;export&#x2F;server&#x2F;kafka&#x2F;confifig 修改 配置文件 server.properties</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/kafka/config</span><br><span class="line"></span><br><span class="line">vim server.properties</span><br></pre></td></tr></table></figure>

<p>①21 行内容 broker.id&#x3D;0 为依次增长的:0、1、2、3、4,集群中唯一 id 从0开始，每台不能重复<br><img src="/../images/%E5%9B%BE%E7%89%8712.png"><br>②31 行内容 #listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;:9092 取消注释，改为listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092<br><img src="/../images/%E5%9B%BE%E7%89%8713.png"><br>③59 行内容 log.dirs&#x3D;&#x2F;tmp&#x2F;kafka-logs 为默认日志文件存储的位置，改为log.dirs&#x3D;&#x2F;export&#x2F;server&#x2F;data&#x2F;kafka-logs<br><img src="/../images/%E5%9B%BE%E7%89%8714.png"><br>④63 行内容为 num.partitions&#x3D;1 是默认分区数<br><img src="/../images/%E5%9B%BE%E7%89%8715.png"><br>⑤76 行部分 (数据安全性（持久化之前先放到缓存上，从缓存刷到磁盘上interval.messages interval.ms )<br><img src="/../images/%E5%9B%BE%E7%89%8716.png"><br>⑥93 行部分（数据保留策略 168&#x2F;24&#x3D;7，1073741824&#x2F;1024&#x3D;1GB，300000ms &#x3D; 300s &#x3D; 5min超过了删掉<br>（最后修改时间还是创建时间–&gt;日志段中最晚的一条消息，维护这个最大的时间戳–&gt;用户无法干预）<br><img src="/../images/%E5%9B%BE%E7%89%8717.png"><br>⑦121 行内容 zookeeper.connect&#x3D;localhost:2181 修改为 zookeeper.connect&#x3D;node1:2181,node2:2181，node3:2181<br><img src="/../images/%E5%9B%BE%E7%89%8718.png"><br>⑧126 行内容 group.initial.rebalance.delay.ms&#x3D;0 修改为 group.initial.rebalance.delay.ms&#x3D;3000<br><img src="/../images/%E5%9B%BE%E7%89%8719.png"></p>
<p>·将kafka分发给node2、node3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line"></span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node2:<span class="variable">$PWD</span></span><br><span class="line"></span><br><span class="line">scp -r /export/server/kafka_2.11-2.0.0/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>

<p>·创建软件连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s .<span class="built_in">export</span>/server/kafka_2.11-2.0.0/ kafka</span><br></pre></td></tr></table></figure>

<p>·配置 kafka 环境变量（注：可以一台一台配，也可以在 node1 完成后发给 node2和node3）<br><img src="/../images/%E5%9B%BE%E7%89%8720.png"></p>
<p>·重新加载环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">souce /etc/profile</span><br></pre></td></tr></table></figure>

<p>·修改node02和node03中的service.properties中的broker.id</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/kafka/config</span><br></pre></td></tr></table></figure>
<p>①将文件 server.properties 的第 21 行的 broker.id&#x3D;0 修改为 broker.id&#x3D;1 同理 node2 同样操作<br><img src="/../images/%E5%9B%BE%E7%89%8721.png"><br>②将文件 server.properties 的第 31 行的 listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node1:9092 修改为listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node2:9092 同理node3 同样操作listeners&#x3D;PLAINTEXT:&#x2F;&#x2F;node3:9092</p>
<p> ·启停 kafka (注：kafka 启动需要在 zookeeper 启动的情况下才可)<br><img src="/../images/%E5%9B%BE%E7%89%8722.png"></p>
<p>·定制脚本一键启动<br><img src="/../images/%E5%9B%BE%E7%89%8723.png"></p>
<p>·放入 &#x2F;bin 路径下<br><img src="/../images/%E5%9B%BE%E7%89%8724.png"><br><img src="/../images/%E5%9B%BE%E7%89%8725.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/06/05/kafka-1/" data-id="cl4me0aod0003gwv9bnh2csl8" data-title="kafka 环境配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-My-New" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/21/My-New/" class="article-date">
  <time class="dt-published" datetime="2022-05-21T05:44:36.000Z" itemprop="datePublished">2022-05-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/21/My-New/">Spark HA &amp; Yarn配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一.Spark HA</p>
<p>·启动zookeeper和hdfs</p>
<p>·进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 文件夹 修改 spark-env.sh 文件内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/conf </span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">##指定spark老大Master的IP和提交任务的通信端口</span></span><br><span class="line"><span class="comment">#告知Spark的master运行在哪个机器上</span></span><br><span class="line"><span class="comment">#export SPARK_MASTER__HOST=node1</span></span><br><span class="line"><span class="comment">#告知sparkmnaster的通讯端口</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line"><span class="comment">#告知spark master的webui端 口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"></span><br><span class="line">SPARK_ DAEMON_ JAVA_ OPTS=<span class="string">&quot; -Dapark. deploy. recoveryMode= ZOOKEEPER -Dspark.deploy . zookeeper.</span></span><br><span class="line"><span class="string">url=node1:2181, node2:2181, node3:2181  -Dspark.deploy. zookeeper.dir-/spark-ha&quot;</span></span><br><span class="line"><span class="comment">#spark. deploy. recoveryMode 指定HA模式 基于Zookeeper实现</span></span><br><span class="line"><span class="comment">#指定Zookeeper的连接地址</span></span><br><span class="line"><span class="comment">#指定在Zookeeper中注册临时结点的路径</span></span><br></pre></td></tr></table></figure>


<p>·使用scp分发spark-env.sh到node2,node3上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp spark-env.sh node2:/export/server/spark/conf/</span><br><span class="line"></span><br><span class="line">scp spark-env.sh node3:/export/server/spark/conf/</span><br></pre></td></tr></table></figure>
<p>·更换zookeeper版本</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">将zookeepe新版本压缩包上传到/export/server/路径下，解压后删除压缩包</span><br><span class="line">在 /export/server 目录下创建软连接</span><br><span class="line">在<span class="built_in">cd</span> /export/server/apache-zookeeper-3.7.0-bin/conf/路径下进行文件配置</span><br><span class="line">将zoo_sample.cfg复制为zoo.cfg：<span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br><span class="line">在/export/server/zookeeper/路径下新建zkdatas文件夹：<span class="built_in">mkdir</span> -p /export/server/zookeeper/zkdatas/</span><br></pre></td></tr></table></figure>

<p>·编辑zoo.cfg</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/export/ server/ zookeeper / zkdatas</span><br><span class="line"></span><br><span class="line"><span class="comment">#集群中服务器地址</span></span><br><span class="line"></span><br><span class="line">server.1=node1:2888:3888 </span><br><span class="line">server.2=node2:2888:3888 </span><br><span class="line">server.3=node3:2888:3888</span><br></pre></td></tr></table></figure>

<p>·添加node1的myid配置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /export/server/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p>·使用scp拷贝新版本zookeeper到node2,node3上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/zookeeper-3.7.0/ node2:<span class="variable">$PWD</span></span><br><span class="line"></span><br><span class="line">scp -r /export/server/zookeeper-3.7.0/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>

<p>·添加node2、node3的myid配置</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 2 &gt; /export/server/zookeeper/zkdatas/myid   </span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 3 &gt; /export/server/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p>·重新在&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;bin&#x2F;目录下编写脚本一键启动</p>
<p>·在 master 上 启动一个master 和全部worke</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/spark/sbin/start-all.sh</span><br></pre></td></tr></table></figure>

<p>·访问 WebUI 界面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node1:8081/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%877.png"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node2:8082/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%878.png"></p>
<p>·启动node2上的master</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/export/server/spark/sbin/start-master.sh</span><br></pre></td></tr></table></figure>

<p>·将node1上的master kill掉</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 端口号</span><br></pre></td></tr></table></figure>

<p>·访问node2的spark web UI页面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node2:8082/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%879.png"></p>
<p>二.Spark(YARN)</p>
<p>·在spark-env.sh中配置HADOOP_CONF_DIR和YARN_CONF_DIR（前面配置spark-Standlone 时已经配置过此项了)</p>
<p>·连接到YARN中</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/pyspark --master yarn --deploy-mode client|cluster</span><br><span class="line"><span class="comment"># --deploy-mode 选项是指定部署模式, 默认是 客户端模式</span></span><br><span class="line"><span class="comment"># client就是客户端模式</span></span><br><span class="line"><span class="comment"># cluster就是集群模式</span></span><br><span class="line"><span class="comment"># --deploy-mode 仅可以用在YARN模式下</span></span><br><span class="line"></span><br><span class="line">bin/spark-shell --master yarn --deploy-mode client|cluster</span><br><span class="line"></span><br><span class="line">bin/spark-submit --master yarn --deploy-mode client|cluster /xxx/xxx/xxx.py 参数</span><br></pre></td></tr></table></figure>
<p>·客户端工具</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">·bin/pyspark: pyspark解释器spark环境</span><br><span class="line">·bin/spark-shell: scala解释器spark环境</span><br><span class="line">·bin/spark-submit: 提交jar包或Python文件执行的工具</span><br><span class="line">·bin/spark-sql: sparksql客户端工具</span><br></pre></td></tr></table></figure>

<p>·以spark-submit 为例</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">bin/spark-submit --master spark://master:7077 xxx.py</span><br><span class="line"></span><br><span class="line">Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments]</span><br><span class="line">Usage: spark-submit --<span class="built_in">kill</span> [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit --status [submission ID] --master [spark://...]</span><br><span class="line">Usage: spark-submit run-example [options] example-class [example args]</span><br><span class="line"></span><br><span class="line">Options:</span><br><span class="line">  --master MASTER_URL                      spark://host:port, mesos://host:port, yarn,</span><br><span class="line">                                                            k8s://https://host:port, or <span class="built_in">local</span> (Default: <span class="built_in">local</span>[*]).</span><br><span class="line">  --deploy-mode DEPLOY_MODE        部署模式 client 或者 cluster 默认是client</span><br><span class="line">  --class CLASS_NAME                         运行java或者scala class(<span class="keyword">for</span> Java / Scala apps).</span><br><span class="line">  --name NAME                                   程序的名字</span><br><span class="line">  --jars JARS                                         Comma-separated list of jars to include on the </span><br><span class="line">driver</span><br><span class="line">                                                            and executor classpaths.</span><br><span class="line">  --packages                                        Comma-separated list of maven coordinates of</span><br><span class="line"> jars to include</span><br><span class="line">                                                            on the driver and executor classpaths. Will </span><br><span class="line">search the <span class="built_in">local</span></span><br><span class="line">                                                            maven repo, <span class="keyword">then</span> maven central and any </span><br><span class="line">additional remote</span><br><span class="line">                                                            repositories given by --repositories. The format </span><br><span class="line"><span class="keyword">for</span> the</span><br><span class="line">                                                            coordinates should be </span><br><span class="line">groupId:artifactId:version.</span><br><span class="line">   --exclude-packages                         Comma-separated list of groupId:artifactId, to </span><br><span class="line">exclude <span class="keyword">while</span></span><br><span class="line">                                                            resolving the dependencies provided <span class="keyword">in</span> --</span><br><span class="line">packages to avoid</span><br><span class="line">                                                            dependency conflicts.</span><br><span class="line">  --repositories                                    Comma-separated list of additional remote </span><br><span class="line">repositories to</span><br><span class="line">                                                            search <span class="keyword">for</span> the maven coordinates given with --</span><br><span class="line">packages.</span><br><span class="line">  --py-files PY_FILES                            指定Python程序依赖的其它python文件</span><br><span class="line">  --files FILES                                       Comma-separated list of files to be placed <span class="keyword">in</span> </span><br><span class="line">the working</span><br><span class="line">                                                            directory of each executor. File paths of these </span><br><span class="line">files</span><br><span class="line">                                                            <span class="keyword">in</span> executors can be accessed via </span><br><span class="line">SparkFiles.get(fileName).</span><br><span class="line">  --archives ARCHIVES                        Comma-separated list of archives to be extracted </span><br><span class="line">into the</span><br><span class="line">                                                           working directory of each executor.</span><br><span class="line"></span><br><span class="line">  --conf, -c PROP=VALUE                   手动指定配置</span><br><span class="line">  --properties-file FILE                        Path to a file from <span class="built_in">which</span> to load extra </span><br><span class="line">properties. If not</span><br><span class="line">                                                           specified, this will look <span class="keyword">for</span> conf/spark-</span><br><span class="line">defaults.conf.</span><br><span class="line"></span><br><span class="line">  --driver-memory MEM                     Driver的可用内存(Default: 1024M).</span><br><span class="line">  --driver-java-options                       Driver的一些Java选项</span><br><span class="line">  --driver-library-path                        Extra library path entries to pass to the </span><br><span class="line">driver.</span><br><span class="line">  --driver-class-path                          Extra class path entries to pass to the driver. </span><br><span class="line">Note that</span><br><span class="line">                                                           jars added with --jars are automatically</span><br><span class="line">included <span class="keyword">in</span> the</span><br><span class="line">                                                          classpath.</span><br><span class="line"></span><br><span class="line">  --executor-memory MEM                Executor的内存 (Default: 1G).</span><br><span class="line"></span><br><span class="line">  --proxy-user NAME                         User to impersonate when submitting the </span><br><span class="line">application.</span><br><span class="line">                                                          This argument does not work with --principal / -</span><br><span class="line">-keytab.</span><br><span class="line"></span><br><span class="line">  --<span class="built_in">help</span>, -h                                        显示帮助文件</span><br><span class="line">  --verbose, -v                                   Print additional debug output.</span><br><span class="line">  --version,                                        打印版本</span><br><span class="line"></span><br><span class="line"> Cluster deploy mode only(集群模式专属):</span><br><span class="line">  --driver-cores NUM                        Driver可用的的CPU核数(Default: 1).</span><br><span class="line"></span><br><span class="line"> Spark standalone or Mesos with cluster deploy mode only:</span><br><span class="line">  --supervise                                     如果给定, 可以尝试重启Driver</span><br><span class="line"></span><br><span class="line"> Spark standalone, Mesos or K8s with cluster deploy mode only:</span><br><span class="line">  --<span class="built_in">kill</span> SUBMISSION_ID                    指定程序ID <span class="built_in">kill</span></span><br><span class="line">  --status SUBMISSION_ID               指定程序ID 查看运行状态</span><br><span class="line"></span><br><span class="line"> Spark standalone, Mesos and Kubernetes only:</span><br><span class="line">  --total-executor-cores NUM         整个任务可以给Executor多少个CPU核心用</span><br><span class="line"></span><br><span class="line"> Spark standalone, YARN and Kubernetes only:</span><br><span class="line">  --executor-cores NUM                 单个Executor能使用多少CPU核心</span><br><span class="line"></span><br><span class="line"> Spark on YARN and Kubernetes only(YARN模式下):</span><br><span class="line">  --num-executors NUM                 Executor应该开启几个</span><br><span class="line">  --principal PRINCIPAL                  Principal to be used to login to KDC.</span><br><span class="line">  --keytab KEYTAB                          The full path to the file that contains the </span><br><span class="line">keytab <span class="keyword">for</span> the</span><br><span class="line">                                                       principal specified above.</span><br><span class="line"></span><br><span class="line"> Spark on YARN only:</span><br><span class="line">  --queue QUEUE_NAME                指定运行的YARN队列(Default: <span class="string">&quot;default&quot;</span>).</span><br></pre></td></tr></table></figure>

<p>·启动历史服务器</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/hadoop-3.3.0/sbin</span><br><span class="line"></span><br><span class="line">./mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure>

<p>·cilent模式测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> SPARK_ HOME=/export/server/ spark</span><br><span class="line"></span><br><span class="line">$[SPARK_ HOME]/bin/spark-submit --master yarn --dep 1oy-mode client  --driver-memory 512m --executor -memory 512m - -num-executors 1 --total- executor-cores 2 $(SPARK_ _HOME]/examples/src/main/python/pi.py 3</span><br></pre></td></tr></table></figure>

<p>·cluster模式测试</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SPARK_ HOME=/export/ server/ spark</span><br><span class="line"></span><br><span class="line">$[SPARK_ HOME]/bin/ spark-submit --master yarn --deploy-mode clus ter --driver- memory 512m --executor-memory 512m -- num executors 1 --tota 1-executor-cores 2 --conf <span class="string">&quot;spark . pyspark. driver . python=/ root/ anaconda3/bin/python3&quot;</span> --conf <span class="string">&quot;spark. pyspark. python=/ root/anaconda3/bin/python3&quot;</span></span><br><span class="line">$[SPARK_ _HOME]/examples/src/main/python/pi .py 3</span><br></pre></td></tr></table></figure>

<p>·访问WebUI界面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node1:19888/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%8710.png"></p>
<p><img src="/../images/%E5%9B%BE%E7%89%8711.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/21/My-New/" data-id="cl4me0aoc0002gwv9avfc0ruv" data-title="Spark HA &amp; Yarn配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-My-Post" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/20/My-Post/" class="article-date">
  <time class="dt-published" datetime="2022-05-20T14:58:23.000Z" itemprop="datePublished">2022-05-20</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/20/My-Post/">Spark local&amp; stand-alone配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一.Spark local</p>
<p>·安装上传安装包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行文件</span></span><br><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">操作过程:</span><br><span class="line"></span><br><span class="line">Please answer<span class="string">&#x27; yes&#x27;</span> or <span class="string">&#x27;no&#x27;</span>:<span class="string">&#x27;</span></span><br><span class="line"><span class="string"> &gt;&gt;&gt; yes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Anaconda3 will now be installed into this location: </span></span><br><span class="line"><span class="string">/root/anaconda3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     -Press ENTER to confirm the location</span></span><br><span class="line"><span class="string">     -Press CTRL-C to abort the installation </span></span><br><span class="line"><span class="string">     -Or specify a different location below</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[/root/ anaconda3] &gt;&gt;&gt; /export/server/ anaconda3 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">·安装完成后退出终端，重新进入</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">```bash</span></span><br><span class="line"><span class="string">exit</span></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line"></span><br><span class="line"><span class="comment"># 看到这个Base开头表明安装好了.base是默认的虚拟环境.</span></span><br><span class="line">Last login: Tue Mar 15 12:01:32 2022 from 192.168.88.1</span><br><span class="line">(base) [root@master ~]<span class="comment"># </span></span><br></pre></td></tr></table></figure>

<p>·创建pyspark基于python3.8</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 </span><br></pre></td></tr></table></figure>
<p>·切换到虚拟环境内</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark  </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line"></span><br><span class="line">(base) [root@node1 ~]<span class="comment"># conda activate pyspark  </span></span><br><span class="line">(pyspark) [root@node1 ~]<span class="comment"># </span></span><br></pre></td></tr></table></figure>

<p>·在虚拟环境内安装包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<p>·Spark安装并解压</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line">tar -zxvf spark-3.2.0-bin-hadoop3.2.tgz -C /export/server/</span><br></pre></td></tr></table></figure>

<p>·建立软连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<p>·添加环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim .bashrc</span><br><span class="line"></span><br><span class="line"><span class="comment">#JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241  </span><br><span class="line"><span class="comment">#PYSPARK_PYTHON</span></span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/export/server/anaconda3/envs/pyspark/bin/python</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>·重新加载环境变量文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<p>·进入cd &#x2F;export&#x2F;server&#x2F;anaconda3&#x2F;envs&#x2F;pyspark&#x2F;bin&#x2F;开启.&#x2F;pyspark</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/anaconda3/envs/pyspark/bin/</span><br><span class="line"></span><br><span class="line">./pyspark</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line">(base) [root@node1 bin]<span class="comment"># ./pyspark</span></span><br><span class="line">Python 3.8.13 (default, Mar 28 2022, 11:38:47) </span><br><span class="line">[GCC 7.5.0]  ::  Anaconda, Inc. on linux</span><br><span class="line">Type <span class="string">&quot;help&quot;</span>, <span class="string">&quot;copyright&quot;</span>, <span class="string">&quot;credits&quot;</span> or <span class="string">&quot;license&quot;</span> <span class="keyword">for</span> more information.</span><br><span class="line">Setting default <span class="built_in">log</span> level to <span class="string">&quot;WARN&quot;</span>.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">2022-04-12 16:45:58,717 WARN util.NativeCodeLoader: Unable to load native-hadoop library <span class="keyword">for</span> your platform... using builtin-java classes <span class="built_in">where</span> applicable</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  <span class="string">&#x27;_/</span></span><br><span class="line"><span class="string">   /__ / .__/\_,_/_/ /_/\_\   version 3.2.0</span></span><br><span class="line"><span class="string">      /_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Python version 3.8.13 (default, Mar 28 2022 11:38:47)</span></span><br><span class="line"><span class="string">Spark context Web UI available at http://node1:4040</span></span><br><span class="line"><span class="string">Spark context available as &#x27;</span>sc<span class="string">&#x27; (node1 = local[*], app id = local-1649753161876).</span></span><br><span class="line"><span class="string">SparkSession available as &#x27;</span>spark<span class="string">&#x27;.</span></span><br><span class="line"><span class="string">&gt;&gt;&gt; </span></span><br></pre></td></tr></table></figure>

<p>·查看WebUI界面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">浏览器访问：</span><br><span class="line">http://node1:4040/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%873.png"></p>
<p>·退出</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda deactivate</span><br></pre></td></tr></table></figure>

<p>二.stand-alone</p>
<p>·安装上传安装包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行文件</span></span><br><span class="line">sh Anaconda3-2021.05-Linux-x86_64.sh</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">操作过程：</span><br><span class="line"></span><br><span class="line">Please answer<span class="string">&#x27; yes&#x27;</span> or <span class="string">&#x27;no&#x27;</span>:<span class="string">&#x27;</span></span><br><span class="line"><span class="string"> &gt;&gt;&gt; yes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Anaconda3 will now be installed into this location: </span></span><br><span class="line"><span class="string">/root/anaconda3</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     -Press ENTER to confirm the location</span></span><br><span class="line"><span class="string">     -Press CTRL-C to abort the installation </span></span><br><span class="line"><span class="string">     -Or specify a different location below</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[/root/ anaconda3] &gt;&gt;&gt; /export/server/ anaconda3 </span></span><br></pre></td></tr></table></figure>

<p>·安装完成后退出终端，重新进入</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line"></span><br><span class="line"># 看到这个Base开头表明安装好了.base是默认的虚拟环境.</span><br><span class="line">Last login: Tue Mar 15 16:27:45 2022 from 192.168.88.1</span><br><span class="line">(base) [root@node1 ~]# </span><br></pre></td></tr></table></figure>

<p>·在 node1 节点上把 .&#x2F;bashrc 和 profile 分发给 node2 和 node3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分发 .bashrc :</span></span><br><span class="line">scp ~/.bashrc root@node2:~/</span><br><span class="line">scp ~/.bashrc root@node3:~/</span><br><span class="line"></span><br><span class="line"><span class="comment">#分发 profile :</span></span><br><span class="line">scp /etc/profile/ root@node2:/etc/</span><br><span class="line">scp /etc/profile/ root@node3:/etc/</span><br></pre></td></tr></table></figure>

<p>·创建虚拟环境 pyspark 基于 python3.8</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n pyspark python=3.8 </span><br></pre></td></tr></table></figure>

<p>·切换到虚拟环境内</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate pyspark  </span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结果显示：</span><br><span class="line"></span><br><span class="line">(base) [root@node1 ~]<span class="comment"># conda activate pyspark  </span></span><br><span class="line">(pyspark) [root@node1~]<span class="comment"># </span></span><br></pre></td></tr></table></figure>

<p>·在虚拟环境内安装包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pyhive pyspark jieba -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br></pre></td></tr></table></figure>

<p>·node1节点进入 &#x2F;export&#x2F;server&#x2F;spark&#x2F;conf 修改以下配置文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/conf</span><br></pre></td></tr></table></figure>

<p>·将文件 workers.template 改名为 workers，并配置内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> workers.template workers</span><br><span class="line"></span><br><span class="line">vim workers</span><br><span class="line"></span><br><span class="line"><span class="comment"># localhost删除，内容追加文末：</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure>

<p>·将文件 spark-env.sh.template 改名为 spark-env.sh，并配置内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-env.sh.template spark-env.sh</span><br><span class="line"></span><br><span class="line">vim spark-env.sh</span><br><span class="line"></span><br><span class="line">文末追加内容：</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置JAVA安装目录</span></span><br><span class="line">JAVA_HOME=/export/server/jdk</span><br><span class="line"></span><br><span class="line"><span class="comment">## HADOOP软件配置文件目录，读取HDFS上文件和运行YARN集群</span></span><br><span class="line">HADOOP_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/export/server/hadoop/etc/hadoop</span><br><span class="line"></span><br><span class="line"><span class="comment">## 指定spark老大Master的IP和提交任务的通信端口</span></span><br><span class="line"><span class="comment"># 告知Spark的master运行在哪个机器上</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_HOST=node1</span><br><span class="line"><span class="comment"># 告知sparknode1的通讯端口</span></span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_PORT=7078</span><br><span class="line"><span class="comment"># 告知spark master的 webui端口</span></span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line"><span class="comment"># worker cpu可用核数</span></span><br><span class="line">SPARK_WORKER_CORES=1</span><br><span class="line"><span class="comment"># worker可用内存</span></span><br><span class="line">SPARK_WORKER_MEMORY=1g</span><br><span class="line"><span class="comment"># worker的工作通讯地址</span></span><br><span class="line">SPARK_WORKER_PORT=7078</span><br><span class="line"><span class="comment"># worker的 webui地址</span></span><br><span class="line">SPARK_WORKER_WEBUI_PORT=8081</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置历史服务器</span></span><br><span class="line"><span class="comment"># 配置的意思是  将spark程序运行的历史日志 存到hdfs的/sparklog文件夹中</span></span><br><span class="line">SPARK_HISTORY_OPTS=<span class="string">&quot;-Dspark.history.fs.logDirectory=hdfs://node1:8020/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span></span><br></pre></td></tr></table></figure>

<p>·开启 hadoop 的 hdfs 和 yarn 集群</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line"></span><br><span class="line">start-yarn.sh </span><br></pre></td></tr></table></figure>

<p>·在HDFS上创建程序运行历史记录存放的文件夹，同样 conf 文件目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -<span class="built_in">mkdir</span> /sparklog</span><br><span class="line"></span><br><span class="line">hadoop fs -<span class="built_in">chmod</span> 777 /sparklog</span><br></pre></td></tr></table></figure>

<p>·将 spark-defaults.conf.template 改为 spark-defaults.conf ，并配置内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mv</span> spark-defaults.conf.template spark-defaults.conf</span><br><span class="line"></span><br><span class="line">vim spark-defaults.conf</span><br><span class="line"></span><br><span class="line">文末追加内容为：</span><br><span class="line"><span class="comment"># 开启spark的日期记录功能</span></span><br><span class="line">spark.eventLog.enabled 	<span class="literal">true</span></span><br><span class="line"><span class="comment"># 设置spark日志记录的路径</span></span><br><span class="line">spark.eventLog.<span class="built_in">dir</span>	 hdfs://node1:8020/sparklog/ </span><br><span class="line"><span class="comment"># 设置spark日志是否启动压缩</span></span><br><span class="line">spark.eventLog.compress 	<span class="literal">true</span></span><br></pre></td></tr></table></figure>
<p>·将log4j.properties.template改名为log4j.properties，并配置内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">log4j . rootCategory=WAFN, console</span><br><span class="line">log4j . appender .console=org. apache. log4j. ConsoleAppender</span><br><span class="line">log4j . appender . console, target= System.err</span><br><span class="line">log4j . appender . console. layout-org . apache .1og4j. PatternLayout</span><br><span class="line">log4j . appender . console.layout . ConversionPattern=d&#123;yy/MM/dd HH:mm:ss&#125; <span class="string">&#x27;%p %c&#123;1&#125;: %m%n</span></span><br></pre></td></tr></table></figure>

<p>·使用scp拷贝spark到node2,node3上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node2:<span class="variable">$PWD</span></span><br><span class="line"></span><br><span class="line">scp -r /export/server/spark-3.2.0-bin-hadoop3.2/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>

<p>·在node2、node3上给spark安装目录增加软连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s /export/server/spark-3.2.0-bin-hadoop3.2 /export/server/spark</span><br></pre></td></tr></table></figure>

<p>·重新加载环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>·启动历史服务器&#x2F;export&#x2F;server&#x2F;spark&#x2F;sbin&#x2F;start-history-server.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/spark/sbin </span><br><span class="line"></span><br><span class="line">./start-history-server.sh</span><br></pre></td></tr></table></figure>

<p>·启动spark的master和worker进程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>

<p>·查看spark web UI页面（8080）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node1:18080/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%874.png"></p>
<p>·查看历史服务器web UI页面（18080）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://node1:8080/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%875.png"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/20/My-Post/" data-id="cl4me0aoi0006gwv90ulj8cxi" data-title="Spark local&amp; stand-alone配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-My-New-Post" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/18/My-New-Post/" class="article-date">
  <time class="dt-published" datetime="2022-05-18T12:41:03.000Z" itemprop="datePublished">2022-05-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/18/My-New-Post/">Spark基础环境配置</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>一.基础环境</p>
<p>·查看主机名</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> /etc/hostname</span><br></pre></td></tr></table></figure>

<p> ·Host映射</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /etc</span><br><span class="line"><span class="built_in">cat</span> hosts</span><br></pre></td></tr></table></figure>

<p> ·查看防火墙状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status firewalld.service</span><br></pre></td></tr></table></figure>

<p> ·登录node1、node2、node3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh node1</span><br><span class="line">ssh node2</span><br><span class="line">ssh node3</span><br></pre></td></tr></table></figure>

<p> ·同步时间</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate ntp5.aliyun.com</span><br></pre></td></tr></table></figure>

<p>二.jdk配置</p>
<p> ·编译环境软件安装目录</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> -p /export/server</span><br></pre></td></tr></table></figure>

<p>·上传 jdk-8u241-linux-x64.tar.gz到&#x2F;export&#x2F;server&#x2F;目录下 并解压文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u241-linux-x64.tar.gz</span><br></pre></td></tr></table></figure>

<p> ·配置环境变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"> </span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">expore PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin</span><br><span class="line"><span class="built_in">export</span> CLASSPATH=.:<span class="variable">$JAVA_HOME</span>/lib/dt.jar:<span class="variable">$JAVA_HOME</span>/lib/tools.jar</span><br></pre></td></tr></table></figure>

<p> ·重新加载变量</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p> ·查看 java 版本号 </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br></pre></td></tr></table></figure>

<p> ·node1节点将Java分发到node2、node3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node2:/export/server/</span><br><span class="line">scp -r /export/server/jdk1.8.0_241/ root@node3:/export/server/</span><br></pre></td></tr></table></figure>

<p> ·配置 node2 和 node3 的 jdk 环境变量（和上方 node2 的配置方法一样）</p>
<p> ·建立软连接(node1、node2、node3）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s jdk1.8.0_24/ jdk</span><br></pre></td></tr></table></figure>

<p> ·重新加载环境变量文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>三.Hadoop配置  </p>
<p> ·将Hadoop-3.3.0上传至&#x2F;export&#x2F;server目录下并解压源码包，设计软连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.0-Centos7-64-with-snappy.tar.gz</span><br><span class="line"><span class="built_in">ln</span> -s /export/server/hadoop-3.3.0/ /export/server/hadoopf </span><br></pre></td></tr></table></figure>

<p> ·修改配置文件（进入路径 export&#x2F;server&#x2F;hadoop-3.3.0&#x2F;etc&#x2F;hadoop）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/hadoop-3.3.0/etc/hadoop</span><br></pre></td></tr></table></figure>

<p> ·hadoop-env.sh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在文件末尾添加</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/export/server/jdk1.8.0_241</span><br><span class="line">   </span><br><span class="line"><span class="built_in">export</span> HDFS_NAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_DATANODE_USER=root</span><br><span class="line"><span class="built_in">export</span> HDFS_SECONDARYNAMENODE_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_RESOURCEMANAGER_USER=root</span><br><span class="line"><span class="built_in">export</span> YARN_NODEMANAGER_USER=root </span><br></pre></td></tr></table></figure>

<p> ·core-site.xmlm</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置默认使用的文件系统 Hadoop支持file、HDFS、GFS、ali|Amazon云等文件系统 --&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;hdfs://node1:8020&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;!-- 设置Hadoop本地保存数据路径 --&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;/export/data/hadoop-3.3.0&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;!-- 设置HDFS web UI用户身份 --&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;root&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;!-- 整合hive 用户代理设置 --&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br><span class="line">   </span><br><span class="line">   &lt;!-- 文件系统垃圾桶保存时间 --&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p> ·hdfs-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置SNN进程运行机器位置信息 --&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">       &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;node2:9868&lt;/value&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p> ·mapred-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置MR程序默认运行模式： yarn集群模式 <span class="built_in">local</span>本地模式 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- MR程序历史服务地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;node1:10020&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">     </span><br><span class="line">    &lt;!-- MR程序历史服务器web端地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;node1:19888&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.app.mapreduce.am.env&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.map.env&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.reduce.env&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;HADOOP_MAPRED_HOME=<span class="variable">$&#123;HADOOP_HOME&#125;</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p> ·yarn-site.xml</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置YARN集群主角色运行机器位置 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">    	&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    	&lt;value&gt;node1&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 是否将对容器实施物理内存限制 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 是否将对容器实施虚拟内存限制。 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;<span class="literal">false</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 开启日志聚集 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;<span class="literal">true</span>&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 设置yarn历史服务器地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;http://node1:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 历史日志保存的时间 7天 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;604800&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br></pre></td></tr></table></figure>

<p> ·workers</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node1.itcast.cn</span><br><span class="line">node2.itcast.cn</span><br><span class="line">node3.itcast.cn</span><br></pre></td></tr></table></figure>

<p> ·分发同步hadoop安装包</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"> </span><br><span class="line">scp -r hadoop-3.3.0 root@node2:<span class="variable">$PWD</span></span><br><span class="line">scp -r hadoop-3.3.0 root@node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>

<p> ·将hadoop添加到环境变量（3台机器）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"> </span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/export/server/hadoop-3.3.0</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_HOME</span>/bin:<span class="variable">$HADOOP_HOME</span>/sbin</span><br></pre></td></tr></table></figure>

<p> ·重新加载环境变量文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p>· Hadoop集群启动</p>
<p> 首次启动应当格式化namenode</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs namenode -format</span><br></pre></td></tr></table></figure>

<p> 使用脚本一键启动，jps查看进程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh</span><br><span class="line">jps</span><br></pre></td></tr></table></figure>

<p> 查看hdfs、yarn集群的web UI页面</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HDFS集群：http://node1:9870/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%871.png"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">YARN集群：http://node1:8088/</span><br></pre></td></tr></table></figure>
<p><img src="/../images/%E5%9B%BE%E7%89%872.png"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">四.zookeeper配置</span><br><span class="line"></span><br><span class="line">· 配置主机名和IP的映射关系，修改 /etc/hosts 文件，添加 node1.root node2.root  node3.root</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">vim /etc/hosts</span><br><span class="line"></span><br><span class="line">#结果显示</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1              localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line"></span><br><span class="line">192.168.88.151  node1  node1.root</span><br><span class="line">192.168.88.152  node2  node2.root</span><br><span class="line">192.168.88.153  node3  node3.root</span><br></pre></td></tr></table></figure>

<p> ·在node1主机上，解压zookeeper的压缩包到&#x2F;export&#x2F;server路径下去，然后准备进行解压</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server</span><br><span class="line"></span><br><span class="line">tar -zxvf zookeeper-3.4.6.tar.gz</span><br></pre></td></tr></table></figure>

<p> ·建立软链接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper</span><br></pre></td></tr></table></figure>

<p> ·进入 &#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;conf&#x2F; 将 zoo_sample.cfg 文件复制为新文件 zoo.cfg</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/conf/ </span><br><span class="line"><span class="built_in">cp</span> zoo_sample.cfg zoo.cfg</span><br></pre></td></tr></table></figure>

<p> ·给 zoo.cfg 添加内容</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Zookeeper的数据存放目录</span></span><br><span class="line"></span><br><span class="line">dataDir=/export/server/zookeeper/zkdatas</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保留多少个快照</span></span><br><span class="line">autopurge.snapRetainCount=3</span><br><span class="line"></span><br><span class="line"><span class="comment"># 日志多少小时清理一次</span></span><br><span class="line">autopurge.purgeInterval=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#集群中服务器地址</span></span><br><span class="line">server.1=node1: 2888 :3888 </span><br><span class="line">server.2=node2: 2888 :3888 </span><br><span class="line">server.3=node3: 2888 :3888</span><br></pre></td></tr></table></figure>

<p> ·在node1的&#x2F;export&#x2F;server&#x2F;zookeeper&#x2F;zkdatas&#x2F;目录下创建一个myid文件，文件内容为1  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /export/server/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p> ·使用scp拷贝zookeeper安装包到node2,node3上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/</span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ node2:<span class="variable">$PWD</span></span><br><span class="line">scp -r /export/server/zookeeper-3.4.6/ node3:<span class="variable">$PWD</span></span><br></pre></td></tr></table></figure>

<p> ·在node2、node3上建立软连接</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper </span><br><span class="line"><span class="built_in">ln</span> -s zookeeper-3.4.6/ zookeeper </span><br></pre></td></tr></table></figure>

<p> ·修改node2、node3的myid值为2、3</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 2 &gt; /export/server/zookeeper/zkdatas/myid</span><br><span class="line"><span class="built_in">echo</span> 3 &gt; /export/server/zookeeper/zkdatas/myid</span><br></pre></td></tr></table></figure>

<p> ·配置环境变量(三台主机都要配置）</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile</span><br><span class="line"></span><br><span class="line"><span class="comment"># zookeeper 环境变量</span></span><br><span class="line"><span class="built_in">export</span> ZOOKEEPER_HOME=/export/server/zookeeper</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$ZOOKEEPER_HOME</span>/bin</span><br></pre></td></tr></table></figure>

<p> ·重新加载环境变量文件</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure>

<p> ·三台机器启动zookeeper服务</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/server/zookeeper/bin</span><br><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure>

<p> ·三台主机分别查看启动状态</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh  status</span><br></pre></td></tr></table></figure>

<p> ·jps查看进程</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p> ·脚本一键启动</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">vim zkServer.sh</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$#</span> -eq 0 ] ;</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;please input param:start stop&quot;</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> = start  ] ;<span class="keyword">then</span>	</span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing master&quot;</span></span><br><span class="line">	ssh master <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..2&#125;</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping slave<span class="variable">$&#123;i&#125;</span>&quot;</span>	</span><br><span class="line">		ssh slave<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh start&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> = stop ];<span class="keyword">then</span></span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping master &quot;</span></span><br><span class="line">	ssh master <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..2&#125;</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping slave<span class="variable">$&#123;i&#125;</span>&quot;</span>	</span><br><span class="line">		ssh slave<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh stop&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span> = status ];<span class="keyword">then</span></span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ing master&quot;</span></span><br><span class="line">	ssh master <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot;</span></span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..2&#125;</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">	<span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$&#123;1&#125;</span>ping slave<span class="variable">$&#123;i&#125;</span>&quot;</span></span><br><span class="line">	ssh slave<span class="variable">$&#123;i&#125;</span> <span class="string">&quot;source /etc/profile;/export/server/zookeeper/bin/zkServer.sh status&quot;</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<p>·将文件放在 &#x2F;bin 目录下</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x zkServer-all.sh &amp;&amp; zkServer-all.sh</span><br></pre></td></tr></table></figure>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/18/My-New-Post/" data-id="cl4me0ao90001gwv99hqq5r99" data-title="Spark基础环境配置" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/05/15/hello-world/" class="article-date">
  <time class="dt-published" datetime="2022-05-15T05:50:06.090Z" itemprop="datePublished">2022-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2022/05/15/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2022/05/15/hello-world/" data-id="cl4me0aoe0004gwv99pk8fk3b" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  

</section>
        <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/06/20/eagle/">eagle使用功能</a>
          </li>
        
          <li>
            <a href="/2022/06/19/kafka-eagle/">kafka-eagle配置文件</a>
          </li>
        
          <li>
            <a href="/2022/06/05/kafka-3/">kafka API使用方法</a>
          </li>
        
          <li>
            <a href="/2022/06/05/kafka-2/">Kafka命令行操作</a>
          </li>
        
          <li>
            <a href="/2022/06/05/kafka-1/">kafka 环境配置</a>
          </li>
        
      </ul>
    </div>
  </div>

  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/06/">June 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/05/">May 2022</a></li></ul>
    </div>
  </div>

  
</aside>
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2022 By Autoload<br>
      Driven - <a href="https://hexo.io/" target="_blank">Hexo</a>|Theme - <a href="https://github.com/autoload/hexo-theme-auto" target="_blank">Auto</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>


<script src="/js/script.js"></script>




  </div>
</body>
</html>